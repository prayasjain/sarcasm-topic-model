package utils;


import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.util.List;
import java.util.StringTokenizer;

import edu.stanford.nlp.ling.Sentence;
import edu.stanford.nlp.ling.TaggedWord;
import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.process.CoreLabelTokenFactory;
import edu.stanford.nlp.process.DocumentPreprocessor;
import edu.stanford.nlp.process.PTBTokenizer;
import edu.stanford.nlp.process.TokenizerFactory;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;

/** This demo shows user-provided sentences (i.e., {@code List<HasWord>})
 *  being tagged by the tagger. The sentences are generated by direct use
 *  of the DocumentPreprocessor class.
 *
 *  @author Christopher Manning
 */
/*
 * Modified by Aditya J.
 * 
 * Specifically written for the Amazon movie data set. It looks up third column in a document, gets the tags,
 * removes all NNPs and puts the document back together.
 * 
 * Input: 1) model file, 2) Directory where data files are present  3) D: Number of documents
 * 
 * 1) model file = C:\Aditya\POSTaggerModels\english-left3words-distsim.tagger
 * Output goes to directory same as (2) with "input" replaced with "output". This directory must be created
 * beforehand. All files are copied with a ".wordpos" extension.
 * Output format is word/POS.
 */
class POSTagger_Amazon {

	private POSTagger_Amazon() {}

	public static void main(String[] args) throws Exception 
	{
		int D = Integer.parseInt(args[2]);
		
		File[] faFiles = new File(args[1]).listFiles();
		String output = "";

		if (faFiles == null || faFiles.length <= 0){
			System.err.println("Empty or not a directory");
			return;
		}
		/*
		 * Create output directory if it does not exist
		 */
		File outputDir = new File(args[1]);
		if (!outputDir.exists()) {
			System.out.println("Creating directory: " + outputDir.getAbsolutePath());
			boolean result = outputDir.mkdir();  

			if(result) {    
				System.out.println("DIR created");  
			}
		}
		int count = 0;
		POSTagger_Amazon pt;
		pt = new POSTagger_Amazon();
		String modelFile = args[0];
		for(File file: faFiles){

			output = "";
			String outputPath = file.getAbsolutePath();
			
			FileWriter p1, p2, p3;
			
				p1 = new FileWriter(outputPath+".NoNNP");
			
				p2 = new FileWriter(outputPath+".NoNNPNoTags");
				p3 = new FileWriter(outputPath+".OnlySome");
				
			BufferedWriter bw1 = new BufferedWriter(p1);
			BufferedWriter bw2 = new BufferedWriter(p2);
			BufferedWriter bw3 = new BufferedWriter(p3);
			
			if(file.isDirectory()){
				;
			}

			if(file.getName().matches("^(.*?)")){
				System.out.println("Processing "+ file.getAbsolutePath());
			}

			MaxentTagger tagger = new MaxentTagger(modelFile);
			TokenizerFactory<CoreLabel> ptbTokenizerFactory = PTBTokenizer.factory(new CoreLabelTokenFactory(),
					"untokenizable=noneKeep");
			BufferedReader r = new BufferedReader(new FileReader(file.getAbsolutePath()));

			//PrintWriter pw = new PrintWriter(new OutputStreamWriter(System.out, "utf-8"));

			for (int i = 0; i < D; i++)
			{
				//	DocumentPreprocessor documentPreprocessor = new DocumentPreprocessor(r.readLine());
				//	documentPreprocessor.setTokenizerFactory(ptbTokenizerFactory);

				String sentence = r.readLine();
				StringTokenizer st = new StringTokenizer(sentence, "\t");

				if (st.countTokens()!= 3)
					continue;
				
				String token1= st.nextToken();
				String token2 = st.nextToken();
				String token3 = st.nextToken();


				String tSentence = tagger.tagString(token3);
				String tSentence_1 = "", tSentence_2 = "";
				/*
				 * Eliminate all NNP. If you want to dump any other tags, add them here.
				 */
				tSentence_1 = tSentence.replaceAll("[a-zA-Z]*_NNP","");
								
				tSentence_2 = tSentence_1.replaceAll("_[A-Z]*","");
				
				String tSentence_3 = "";
				StringTokenizer st2 = new StringTokenizer(tSentence_1," ");
				while (st2.hasMoreTokens())
				{
					String curr_word = st2.nextToken();
					
					if (curr_word.contains("_JJ") || curr_word.contains("_NN") || curr_word.contains("NNS") || curr_word.contains("RB"))
						tSentence_3 = tSentence_3 + curr_word+" ";
				}
				tSentence_3 = tSentence_3.replaceAll("_[A-Z]*","");
				bw3.write(token1+"\t"+token2+"\t"+tSentence_3);
				bw3.newLine();
				
				bw1.write(token1+"\t"+token2+"\t"+tSentence_1);
				bw1.newLine();

				bw2.write(token1+"\t"+token2+"\t"+tSentence_2);
				bw2.newLine();
	
				}
			
			count++;



			// bw.write(output);
			bw1.close();
			p1.close();
			bw2.close();
			p2.close();
			bw3.close();
			p3.close();
		}

		System.out.println("Total : " +count);
	}
}
